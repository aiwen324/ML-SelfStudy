{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing RNNs and LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('/Users/ssydasheng/anaconda3/envs/cp3/lib/python3.6/site-packages')\n",
    "import autograd\n",
    "import autograd.misc.optimizers as optim\n",
    "# from autograd import optimizers as optim\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "You may find the following resources helpful for understanding how RNNs and LSTMs work:\n",
    "\n",
    "* [The Unreasonable Effectiveness of RNNs (Andrej Karpathy)](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "* [Recurrent Neural Networks Tutorial (Wild ML)](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)\n",
    "* [Understanding LSTM Networks (Chris Olah)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-Level Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "The vocabulary contains ['L', 't', 'R', 'g', 'B', 'A', 'a', 'W', 'D', 'K', 'c', 'm', 'v', ';', 'p', 'j', 'Z', 'H', 'X', '.', 'U', 's', 'z', ' ', ':', 'V', \"'\", '-', 'u', '?', 'w', '3', 'f', 'G', 'o', 'S', ',', 'x', 'b', 'Y', 'N', 'i', 'I', 'T', 'P', '&', 'Q', '$', 'q', 'M', 'n', 'r', 'd', '!', 'O', 'F', 'y', '\\n', 'l', 'C', 'e', 'k', 'h', 'J', 'E']\n",
      "------------------------------\n",
      "TOTAL NUM CHARACTERS = 1115394\n",
      "NUM UNIQUE CHARACTERS = 65\n",
      "char_to_index {'L': 0, 't': 1, 'R': 2, 'g': 3, 'B': 4, 'A': 5, 'a': 6, 'W': 7, 'D': 8, 'K': 9, 'c': 10, 'm': 11, 'v': 12, ';': 13, 'p': 14, 'j': 15, 'Z': 16, 'H': 17, 'X': 18, '.': 19, 'U': 20, 's': 21, 'z': 22, ' ': 23, ':': 24, 'V': 25, \"'\": 26, '-': 27, 'u': 28, '?': 29, 'w': 30, '3': 31, 'f': 32, 'G': 33, 'o': 34, 'S': 35, ',': 36, 'x': 37, 'b': 38, 'Y': 39, 'N': 40, 'i': 41, 'I': 42, 'T': 43, 'P': 44, '&': 45, 'Q': 46, '$': 47, 'q': 48, 'M': 49, 'n': 50, 'r': 51, 'd': 52, '!': 53, 'O': 54, 'F': 55, 'y': 56, '\\n': 57, 'l': 58, 'C': 59, 'e': 60, 'k': 61, 'h': 62, 'J': 63, 'E': 64}\n"
     ]
    }
   ],
   "source": [
    "# Load the Shakespeare text\n",
    "with open('data/shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"------------------------------\")\n",
    "# Print a sample of the text\n",
    "print(text[:100])\n",
    "data_length = len(text)\n",
    "vocab = list(set(text))\n",
    "vocab_size = len(vocab)   # + 1      # The extra + 1 is for the end-of-string token\n",
    "\n",
    "char_to_index = { char:index for (index,char) in enumerate(vocab) }\n",
    "index_to_char = { index:char for (index,char) in enumerate(vocab) }\n",
    "\n",
    "print(\"The vocabulary contains {}\".format(vocab))\n",
    "print(\"------------------------------\")\n",
    "print(\"TOTAL NUM CHARACTERS = {}\".format(data_length))\n",
    "print(\"NUM UNIQUE CHARACTERS = {}\".format(vocab_size))\n",
    "print('char_to_index {}'.format(char_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "\n",
    "![Recurrent Neural Network Diagram](data/rnn.jpg)\n",
    "(Image from the [Wild ML RNN Tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/))\n",
    "\n",
    "The update of an RNN is expressed by the following formulas:\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(U x_t + W h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_t = \\text{softmax}(V h_t + b_y)\n",
    "$$\n",
    "\n",
    "Here, each $x_t$ is a _character_---in this example, there are 65 unique characters. Since in each step the model takes as input a character and outputs a prediction for the next character in the sequence, both $x_t$ and $o_t$ are 65-dimensional vectors, i.e., $x_t, o_t \\in \\mathbb{R}^{65}$. We can choose any dimension for the hidden state $h_t$; in this case, we will use $h_t \\in \\mathbb{R}^{100}$. With this setup, the dimensions of $U$, $W$, and $V$ are $100 \\times 65$, $100 \\times 100$, and $65 \\times 100$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a vector $\\mathbf{x}$, we have:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(\\mathbf{x})_i = \\frac{e^{\\mathbf{x}_i}}{\\sum_j e^{\\mathbf{x}_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerically stable version\n",
    "def softmax(x):\n",
    "    exponential = np.exp(x - np.max(x))\n",
    "    return exponential / np.sum(exponential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(input_size, hidden_size, output_size):\n",
    "    params = {\n",
    "        'U': np.random.randn(hidden_size, input_size) * 0.01,\n",
    "        'W': np.random.randn(hidden_size, hidden_size) * 0.01,\n",
    "        'V': np.random.randn(output_size, hidden_size) * 0.01,\n",
    "        'b_h': np.zeros(hidden_size),\n",
    "        'b_o': np.zeros(output_size)\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_hidden(hidden_size):\n",
    "    return np.zeros(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-06b0ab06fab5>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-06b0ab06fab5>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    h = # TODO\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def model(params, x, h_prev):\n",
    "    h = # TODO\n",
    "    y = # TODO\n",
    "    return y, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(output, target):\n",
    "    \"\"\"Negative log-likelihood loss. Useful for training a classification problem with n classes.\n",
    "    \"\"\"\n",
    "    output = np.log(output)\n",
    "    return -output[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-12-6e093588e8f1>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-6e093588e8f1>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    return loss\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def loss(params, input_seq, target_seq, opts):\n",
    "    \"\"\"\n",
    "    Compute the loss of RNN based on data.\n",
    "    \n",
    "    :param params: dict of str: tensor, including keys U, W, v, b_h, b_o.\n",
    "    :param input_seq: list of str. Input string.\n",
    "    :param target_seq: list of str. Target string.\n",
    "    :param opts: dict of str: int. Including keys input_size, hidden_size, output_size.\n",
    "    \n",
    "    :return final_string: str. \n",
    "    \"\"\"\n",
    "    hidden = initialize_hidden(opts['hidden_size'])\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(len(input_seq)):\n",
    "        #TODO\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b91dc6cafb01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "loss_grad = grad(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def sgd(grad, init_params, callback=None, num_iters=200, step_size=0.1, mass=0.9):\n",
    "    \"\"\"Stochastic gradient descent with momentum.\n",
    "    grad() must have signature grad(x, i), where i is the iteration number.\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot(j, length):\n",
    "    vec = np.zeros(length)\n",
    "    vec[j] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(params, initial, length, opts):\n",
    "    \"\"\"\n",
    "    Sampling a string with a Recurrent neural network.\n",
    "    \n",
    "    :param params: dict of str: tensor, including keys U, W, v, b_h, b_o\n",
    "    :param initial: str. Beginning character.\n",
    "    :param length: length of the generated string.\n",
    "    :param opts: dict of str: int. Including keys input_size, hidden_size, output_size.\n",
    "    \n",
    "    :return final_string: str. \n",
    "    \"\"\"\n",
    "    hidden = initialize_hidden(opts['hidden_size'])\n",
    "    current_char = initial\n",
    "    final_string = initial\n",
    "    \n",
    "    for i in range(length):\n",
    "        x = create_one_hot(char_to_index[current_char], opts['input_size'])\n",
    "        output, hidden = model(params, x, hidden)\n",
    "        \n",
    "        p = output\n",
    "        current_index = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        current_char = index_to_char[current_index]\n",
    "        final_string += current_char\n",
    "    \n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Use non-overlapping 25-character chunks for training\n",
    "    sequence_length = 50\n",
    "    num_epochs = 1\n",
    "    print_every = 100\n",
    "    evaluate_every = 100\n",
    "    lr = 1e-2\n",
    "\n",
    "    opts = {\n",
    "        'input_size': vocab_size,\n",
    "        'hidden_size': 100,\n",
    "        'output_size': vocab_size,\n",
    "    }\n",
    "\n",
    "    params = initialize_params(opts['input_size'], opts['hidden_size'], opts['output_size'])\n",
    "\n",
    "    for ep in range(num_epochs):\n",
    "        # i = 0\n",
    "        # while i * sequence_length + 1 < 10000:\n",
    "        for i in range(data_length // sequence_length):\n",
    "            start = i * sequence_length\n",
    "            end = start + sequence_length + 1\n",
    "            chunk = text[start:end]\n",
    "\n",
    "            input_chars = chunk[:-1]\n",
    "            target_chars = chunk[1:]\n",
    "\n",
    "            input_seq = [char_to_index[c] for c in input_chars]\n",
    "            target_seq = [char_to_index[c] for c in target_chars]\n",
    "\n",
    "            input_seq_one_hot = [create_one_hot(j, vocab_size) for j in input_seq]\n",
    "\n",
    "            example_loss = loss(params, input_seq_one_hot, target_seq, opts)\n",
    "\n",
    "            grad_params = loss_grad(params, input_seq_one_hot, target_seq, opts)\n",
    "            for param in params:\n",
    "                gradient = np.clip(grad_params[param], -5, 5)\n",
    "                params[param] -= lr * gradient\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                print(\"LOSS = {}\".format(example_loss))\n",
    "                # print(grad_params)\n",
    "\n",
    "            if i % evaluate_every == 0:\n",
    "                sampled_string = sample(params, initial='a', length=100, opts=opts)\n",
    "                print(sampled_string)\n",
    "\n",
    "            # i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-0ae8f86dbf05>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0minput_seq_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcreate_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mexample_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_seq_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mgrad_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_seq_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory Networks (LSTMs)\n",
    "\n",
    "![Long Short-Term Memory Networks Diagram](data/LSTM.png)\n",
    "(Image from the [LSTM Tutorial](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update of an LSTM is given by the following equations:\n",
    "\n",
    "$$\n",
    "i_t = \\sigma(U_i x_t + W_i h_{t-1} + b_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_t = \\sigma(U_f x_t + W_f h_{t-1} + b_f)\n",
    "$$\n",
    "\n",
    "$$\n",
    "o_t = \\sigma(U_o x_t + W_o h_{t-1} + b_o)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{C}_t = \\tanh(U_C x_t + W_C h_{t-1} + b_C)\n",
    "$$\n",
    "\n",
    "$$\n",
    "C_t = i_t * \\tilde{C}_t + f_t * C_{t-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_t = o_t * \\tanh(C_t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(input_size, hidden_size, output_size):\n",
    "    params = {\n",
    "        'U_i': np.random.randn(hidden_size, input_size) * 0.01,\n",
    "        'W_i': np.random.randn(hidden_size, hidden_size) * 0.01,\n",
    "        'b_i': np.zeros(hidden_size),\n",
    "        \n",
    "        'U_f': np.random.randn(hidden_size, input_size) * 0.01,\n",
    "        'W_f': np.random.randn(hidden_size, hidden_size) * 0.01,\n",
    "        'b_f': np.zeros(hidden_size),\n",
    "        \n",
    "        'U_o': np.random.randn(hidden_size, input_size) * 0.01,\n",
    "        'W_o': np.random.randn(hidden_size, hidden_size) * 0.01,\n",
    "        'b_o': np.zeros(hidden_size),\n",
    "        \n",
    "        'U_c': np.random.randn(hidden_size, input_size) * 0.01,\n",
    "        'W_c': np.random.randn(hidden_size, hidden_size) * 0.01,\n",
    "        'b_c': np.zeros(hidden_size),\n",
    "        \n",
    "        'V': np.random.randn(output_size, hidden_size) * 0.01,\n",
    "        'b': np.zeros(output_size)\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "def model(params, x, h_prev, C_prev):\n",
    "    # Notice don't use softmax here, each output is independent when we calculating the \n",
    "    # forget gate, input gate and output gate.\n",
    "    i_t = sigmoid(np.dot(params['U_i'], x) + np.dot(params['W_i'], h_prev) + params['b_i'])\n",
    "    f_t = sigmoid(np.dot(params['U_f'], x) + np.dot(params['W_f'], h_prev) + params['b_f'])\n",
    "    o_t = sigmoid(np.dot(params['U_o'], x) + np.dot(params['W_o'], h_prev) + params['b_o'])\n",
    "    \n",
    "    C_t_tilde = np.tanh(np.dot(params['U_c'], x) + np.dot(params['W_c'], h_prev) + params['b_c'])\n",
    "    C_t = C_t_tilde * i_t + C_prev * f_t\n",
    "    h_t = np.tanh(C_t) * o_t\n",
    "    \n",
    "    y = softmax(np.dot(params['V'], h_t) + params['b'])\n",
    "    return y, h_t, C_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_hidden(hidden_size):\n",
    "    return np.zeros(hidden_size), np.zeros(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, input_seq, target_seq, opts):\n",
    "    \"\"\"\n",
    "    Compute the loss of RNN based on data.\n",
    "    \n",
    "    :param params: dict of str: tensor, including keys U, W, v, b_h, b_o.\n",
    "    :param input_seq: list of str. Input string.\n",
    "    :param target_seq: list of str. Target string.\n",
    "    :param opts: dict of str: int. Including keys input_size, hidden_size, output_size.\n",
    "    \n",
    "    :return final_string: str. \n",
    "    \"\"\"\n",
    "    hidden, cell = initialize_hidden(opts['hidden_size'])\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(len(input_seq)):\n",
    "        y, hidden, cell = model(params, input_seq[i], hidden, cell)\n",
    "        loss += -np.log(y[target_seq[i]])\n",
    "    return loss\n",
    "\n",
    "loss_grad = grad(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.69314718, 1.09861229])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(np.array([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(params, initial, length, opts):\n",
    "    \"\"\"\n",
    "    Sampling a string with a Recurrent neural network.\n",
    "    \n",
    "    :param params: dict of str: tensor, including keys U, W, v, b_h, b_o\n",
    "    :param initial: str. Beginning character.\n",
    "    :param length: length of the generated string.\n",
    "    :param opts: dict of str: int. Including keys input_size, hidden_size, output_size.\n",
    "    \n",
    "    :return final_string: str. \n",
    "    \"\"\"\n",
    "    hidden, cell = initialize_hidden(opts['hidden_size'])\n",
    "    current_char = initial\n",
    "    final_string = initial\n",
    "    \n",
    "    for i in range(length):\n",
    "        x = create_one_hot(char_to_index[current_char], opts['input_size'])\n",
    "        output, hidden, cell = model(params, x, hidden, cell)\n",
    "        \n",
    "        p = output\n",
    "        current_index = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        current_char = index_to_char[current_index]\n",
    "        final_string += current_char\n",
    "    \n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS = 208.71676583425688\n",
      "aMdlsgIqsF!LXE-clPfb,VwUuiMIwhLFB&uPwCW-C,zArH\n",
      "xb&pt$yPV;$NbVr\n",
      ";?Onv&T$:uo?JJfy'xdf?\n",
      "i'M!HFKmM:bZfucp\n",
      "LOSS = 156.92519525656888\n",
      "asg\n",
      "hfb:ysgrf!eTaAPtbauielrsY  h'cottN'dHNofmlei'n'eeeoienhh em\n",
      "mtTSr 'eqoidv'd aorwu;Ri qHehnsXeo r \n",
      "LOSS = 155.59223497779118\n",
      "aCa.\n",
      " rr\n",
      "fyAybxshtharnv o  rt!gr\n",
      "se,nh,n\n",
      "  t hhlisrrstty n lguLvieesiohktJathE\n",
      "a ntuhed o siasFeeui\n",
      "c\n",
      "LOSS = 154.84366703216585\n",
      "at g'ec:im\n",
      " oi\n",
      "sgfh yU\n",
      "sw rn h neust onsef  wh\n",
      ",st .k-nrnh\n",
      "nee  uen  yu\n",
      "bymhh,,ttIoraaotS  k solrssag\n",
      "LOSS = 171.97262106866316\n",
      "aFusseAitb,oNIsh bmd\n",
      "tnroO sN  wo  n\n",
      "   a f.ri,westIdiae:R!sfe y sw ,r wIUsItT,d.aieaeln lad s;tpen r\n",
      "LOSS = 156.63381566862287\n",
      "auwe,v ?il ,ynoecInn,:nNe s? aar 3\n",
      "e\n",
      "stohnnad, rhisv  .  esy dLWerTyn::usrrye \n",
      "ts bAm-i dric GnmaC.ae\n",
      "LOSS = 150.24414570242664\n",
      "aesf'ro\n",
      "cotm;.u\n",
      "acvS tBo\n",
      "otrOrdloirz  rdItestith ee Qe d\n",
      "tmt.a s\n",
      "Wdy tl'o'.  u yoaseeie\n",
      " ya oe,oar eo\n",
      "LOSS = 144.80467641580697\n",
      "at\n",
      "sc uir coe uese,dRn lI t ue hrw u,t  afumi ha otoe  sawe enteeoeoa fri ts Le yS Iyb tltf oWdgjlol,\n",
      "LOSS = 169.53864348282704\n",
      "ahoker bRele adncac our o:hmrh srSe seioa\n",
      "n CHmahars'e'd ,ae caae yofrcihfeau a\n",
      "ptL d$gb:eRrIb utrLth\n",
      "LOSS = 145.19055208159693\n",
      "aoacn reiao\n",
      "\n",
      "a nh m riioIin irid pa\n",
      "ane\n",
      "iyue sm?dpetrPh,k , aard ohs hn Io, hwry\n",
      " esc hcd hrmsr\n",
      "i:vuI\n",
      "LOSS = 143.2401455986668\n",
      "aen 'beo Icsss\n",
      "hone\n",
      "Mriekmr nrdloli wHoa ot\n",
      "wr anro tsg,e\n",
      "\n",
      "Uzd ioe ad nrlufd doe hogdd tra t-c t hr,i\n",
      "LOSS = 136.4437698282298\n",
      "a wa ome\n",
      ": ur ary uad toro hhcte noods: te fe bbv oiber oas hulUonEswy np nn s tto osdrp\n",
      "IAs anmo\n",
      " ms\n",
      "LOSS = 152.70432476269625\n",
      "aerk weore o nonr beirry\n",
      "ta; hhe :ompisvrhy vnompoI .wM padedues i.here hyetSs I i th Ct, Fyiatltev\n",
      "c\n",
      "LOSS = 135.28055357016234\n",
      "ain slo,o thanwos ro yei\n",
      "te. thadipnthertheuoy slrlpad d oaici.e'r foaue otedr shoss ooosatherel rale\n",
      "LOSS = 135.8078512722026\n",
      "adrM gamut alld atecrame worlss'Nar.\n",
      "GC\n",
      "hi:, cur honsl,\n",
      "bhou rreimed,\n",
      "dald\n",
      "\n",
      ";aWhe laced oratcd uuefpE\n",
      "LOSS = 146.9684090744544\n",
      "aB whe mue w;eis ncegtE:\n",
      "\n",
      "T'gtOdTemt :et\n",
      "Tho sel voyelM iwre Iwrgy o the theis so.,\n",
      "\n",
      "I\n",
      "Sso gecmel, tg\n",
      "LOSS = 124.57449573543991\n",
      "aulw\n",
      "ormes tosrlle\n",
      "Zd rCaus mut hhad to swin, ar Iof sam pour het hrsl;ot\n",
      "kens\n",
      "on rhevdrB-gheold oons\n",
      "LOSS = 124.73103220381832\n",
      "ann onlwo hon,\n",
      "banS,,\n",
      "S: 'Nrnnggd she, Tnam bau the her le at to ipr thato ghoe pisus, atoure bu sing\n",
      "LOSS = 116.70026390682327\n",
      "athe'we gornat care tedcindy ancekeg theesthn iwl.\n",
      "\n",
      "EgywI fonpens thoad\n",
      "Bg bire beathenoseet,,\n",
      "\n",
      "CaNUW\n",
      "LOSS = 117.76367102676844\n",
      "a-kil, or hraroned so. Bo cons son mil, nos so theirs\n",
      "\n",
      "Mo're-r eir cheound srsadd uuchit un thiefd iv\n",
      "LOSS = 133.1414623680207\n",
      "ar aat ofteS shew camashd\n",
      "WitrgLoaw wemimd fras.\n",
      "\n",
      "CoNtk mbom:\n",
      "\n",
      "IrCINRNDsUals\n",
      "I,A deammi-, wonchon nor\n",
      "LOSS = 134.60246704896377\n",
      "agesith bcwem:e, itle wheare's wo, thads, fyous. l wanwile hrinond b'es:\n",
      "Mhit sleanssrey sour hepmise\n",
      "LOSS = 128.24035419873172\n",
      "an. momellls fond vnfeles\n",
      "Aw ceel ons mo't sis: mint to to ths hon eres tfomp\n",
      "Tha'llt, I'ly avis, me.\n",
      "LOSS = 111.65913014321117\n",
      "au thant ou cpie cew ive icis, than wer fery ciy uwe shilit ou mi. theeres a sin, dy ateme ag, the gs\n",
      "LOSS = 121.86278606504887\n",
      "ans\n",
      "Her aond: ougsovser sarn:\n",
      "MLawuce.\n",
      "\n",
      "Benoos arit han gouuns inas sar thif meurme, the Moum, fores \n",
      "LOSS = 116.64323647310556\n",
      "aor to apdend tho.\n",
      "\n",
      "Soinn,\n",
      "Se so's heins;\n",
      "Whif tororapr allU-mrcl we Iran's\n",
      "Hen. Tave t. ikesand no T\n",
      "LOSS = 97.7493364142421\n",
      "arcea:', Seey mald he hesbrd.\n",
      "An kewre craty\n",
      "The oflitunler, me soned touH he heid, Iod that thiend\n",
      "T\n",
      "LOSS = 121.01629608832653\n",
      "as mastt. Wom Fak batheonage I walse youn,\n",
      "Fo'd cot foucd ankit ind me, ne g'rat of ''latel;.\n",
      "\n",
      "MEEIMC\n",
      "LOSS = 118.44671634344348\n",
      "at?\n",
      "Vroum warelo the got syemdinitt fyat the nordS\n",
      "Syof Tontane the lorit she to ack theae, porade; t\n",
      "LOSS = 105.64770438680262\n",
      "and widiergenr, at yonm wimed hithen\n",
      "\n",
      "hacllk, bigt lemerek. Yit eere cirgou reseLi- ans tue honk fos \n",
      "LOSS = 116.33792163146249\n",
      "arson\n",
      "Fwibe mapy momegh ofy uolove home hamo,,\n",
      "Fe reme-chur bowh sous; The:\n",
      "Wone so beg; did Mesered,\n",
      "LOSS = 115.1518903176257\n",
      "as sourd\n",
      "Sed,\n",
      "Ht whare,\n",
      "Heard ite, G'Wo mis gousiu hay ins.\n",
      "\n",
      "OwAsdtUL:\n",
      "Ar the mesas iZchs loLgle ard \n",
      "LOSS = 132.85980925983543\n",
      "apOe, Yo arasef? thou you ha'strieshep, thom houve! him thend, yorgind.\n",
      "\n",
      "SNOCDMUNS::\n",
      "Bos moktall wery\n",
      "LOSS = 117.93268144218784\n",
      "an hipcherters frare.\n",
      "$noniis, my hith, nou so weadhiris as nad ung.\n",
      "doult wo Lyou Hwmech this,\n",
      "Ey du\n",
      "LOSS = 114.693089922101\n",
      "and cith fyth thay tomans I cof dneas?\n",
      "Wom sior. the your thyen,\n",
      "Whers a megird dod\n",
      "Hath sofurd tromh\n",
      "LOSS = 99.14469612575915\n",
      "as, Roudh!\n",
      "IO E sunom bors lmeld yor nelrondt, micl anthe, FartI, sull badlss ndamlrcy thle stoungint\n",
      "LOSS = 87.76102548890135\n",
      "ave ay war? of so Pnount-: weid you--en; ar to hat endare eng muke on hor pound,\n",
      "Thas WhaR?eater,\n",
      "IUG\n",
      "LOSS = 104.9535927863688\n",
      "and sarlh not,\n",
      "Surimt in thlust tlous my fa Ep:\n",
      "Gukr stae the ceave obe shmy wing mond thec.\n",
      "\n",
      "ARI CRE\n",
      "LOSS = 104.6079286556071\n",
      "ave thou to harl,\n",
      "Falave of,\n",
      "I cudlcaky tholc inourgithe with lodo s ay m aost a tare daer feery ore \n",
      "LOSS = 104.52352241823087\n",
      "arere, ge weas, un nghat thy he yoord by fy rercom mang so het wither thie Orat scat thak he that the\n",
      "LOSS = 113.43607583311184\n",
      "ald shaideriess And har he wy no?\n",
      "\n",
      "Salts, Eure of decser go deansala be\n",
      "Nintt\n",
      "Sreno tooke is to baoul\n",
      "LOSS = 104.46305340694263\n",
      "ate-t on plolele,\n",
      "I'd mlill I dore wacs pish sruce ie s rlamll, apvy son his shas,\n",
      "Dot yakt huth to p\n",
      "LOSS = 123.53097133855702\n",
      "anggstle\n",
      "\n",
      "wferter corards.\n",
      "The thausthathed at I to fathirgWe,\n",
      "An, doal, dearnoofha tole will iks, no\n",
      "LOSS = 126.71528846521645\n",
      "ar hing worlr you remt ne mw hins datel;\n",
      "so bfpervest henn\n",
      "Gune ce,\n",
      "BEt mipl; henoincngech.\n",
      "\n",
      "HyonkH:\n",
      "\n",
      "LOSS = 107.4435979277486\n",
      "and pate goce.\n",
      "\n",
      "OUOLBFES\n",
      "CLORAU:\n",
      "WarveUL MadE gFoRd:\n",
      "Then theed\n",
      "\n",
      "qallt is ine foth doms the raos, of \n",
      "LOSS = 101.11203211269094\n",
      "am an bwewting en thea co vee ftorn-they nob kiserermty, Geske dit fince.\n",
      "We ciwerd\n",
      "Ceat the brald se\n",
      "LOSS = 120.46965994691361\n",
      "ave, wo s,anou my so ors.\n",
      "\n",
      "POOMCNNTARIN:\n",
      "Pey, Qy todet the to ceet me Call coul, migy,\n",
      "Tour hees?\n",
      "\n",
      "DL\n",
      "LOSS = 111.03440397740485\n",
      "asof thom thou ciile tue strout har im.\n",
      "\n",
      "TAHONKGSH:\n",
      "Oo that se fout OS watrengrold\n",
      "Blous, hit the, th\n",
      "LOSS = 108.22043990921054\n",
      "athef hen saine, I nomeetiren wiing of houd:\n",
      "Or kyaved,\n",
      "Yow lostald weile hese your him ell mie me xe\n",
      "LOSS = 116.68973167445475\n",
      "acde, hapam keakened that taim?\n",
      "\n",
      "WaOd Craverd:\n",
      "Be ble wath at wish of you me, emos midan,\n",
      "I syoof hic\n",
      "LOSS = 104.45820853665955\n",
      "angn.\n",
      "\n",
      "GAUUCGAH:\n",
      "Oy in ind noy yor Eroy ifrerisece biemen wane aven I holiy.\n",
      "Tet lice given,\n",
      "Wis is p\n",
      "LOSS = 108.26126595162269\n",
      "aricungoul\n",
      "Dndeat whin bnostidt,\n",
      "Qove noglepn whoun maist hof lord gnoust meserecl yot misle reaks,\n",
      "T\n",
      "LOSS = 84.92432956770445\n",
      "ace the sill and and you grow me prouk nond babl's you bit that nove we waly sidotel\n",
      "\n",
      "BOMTERTEMAMR:\n",
      "F\n",
      "LOSS = 100.29740413998412\n",
      "ad sinoung;\n",
      "Thas aaters thy hut, lose peace.\n",
      "\n",
      "NLLANN NLIIIEG:\n",
      "Thy, keat; I nom?\n",
      "\n",
      "Host Y:AB:\n",
      "No bemple\n",
      "LOSS = 89.93264359178173\n",
      "at sore the bise?\n",
      "\n",
      "NUCICHIRH:\n",
      "Dhy! owlest shA eunter hish.\n",
      "\n",
      "HANB RIKHARD I I I I coNluty shatt, by pu\n",
      "LOSS = 117.52123210379355\n",
      "ar salt mon wis hir a why lied\n",
      "Noy wirt and low.\n",
      "\n",
      "DEAYE ERIHESDAMc'R:\n",
      "Mades, cathel alld in wimucens.\n",
      "LOSS = 121.64800706286624\n",
      "ad'lly Xuth tucs ghirset,\n",
      "Yint; the shack I veid, me bnere soun worde, deadt whing bat whe nipde scow\n",
      "LOSS = 97.5092481885246\n",
      "and'd thon I ady foous dave of cea,\n",
      "Cergedt amy wamoust.\n",
      "\n",
      "Q IYERES\n",
      "I'e dofIs I handitear os shike on \n",
      "LOSS = 94.41177828132791\n",
      "adien's jarmer, Qlouslins; nost micht-rciintcy\n",
      "Wher lather?\n",
      "The hor blang is my the hible bing Enouls\n",
      "LOSS = 71.87492395089738\n",
      "ad pUunes thill sheri's dand'd made:\n",
      "A the evy beccemfter as to shap efws Yomf Se om bfoneln benery,\n",
      "\n",
      "LOSS = 121.31671601210164\n",
      "alowsand.\n",
      "\n",
      "KING RICZARD RICHAPG EIIHARET:\n",
      "It ficht a kant\n",
      "This kil at lond, wo rim fur Kang.\n",
      "\n",
      "THRRZ\n",
      "A\n",
      "LOSS = 81.76897908256522\n",
      "atten rorky uistrone,\n",
      "and sist.\n",
      "Wellire couml gangshil!\n",
      "Nhice the cofd of repofts'n wileint futor!\n",
      "\n",
      "P\n",
      "LOSS = 87.42966564356004\n",
      "ansuch.\n",
      "Loy Crungre art lead sto, drofu?\n",
      "Whoul, yiouy of mard have.\n",
      "Killitr that thy alliat-.\n",
      "\n",
      "QUSBLO\n",
      "LOSS = 81.29661230940418\n",
      "ack, Ifer do, shore my lorse,\n",
      "Arders in thy a the rost unsep I warnow\n",
      "Fis thy ark with your 'dain ous\n",
      "LOSS = 115.15723720045287\n",
      "aths\n",
      "Word to thyu.\n",
      "\n",
      "BICHNO GZOBCON: Prif tin ven, with timl hived med'l, kili.\n",
      "\n",
      "KING ROCBRMR:\n",
      "O to s \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS = 114.36654710469344\n",
      "ach shis by be pood.\n",
      "Tent eryurt?\n",
      "What a to probly, by the cho theduter of my beap spowbandG, the of \n",
      "LOSS = 110.41701629977783\n",
      "al, thy saked and of Ghags hat meselon hove;\n",
      "Oht wame, bret me archer ally frokent'lllisu sour:\n",
      "Teme \n",
      "LOSS = 100.60204598827508\n",
      "an, Gothtrenvers are for seoy to dees dodarm.\n",
      "\n",
      "DeIlld Sady to bringnste me lose\n",
      "Nour, and andimuse di\n",
      "LOSS = 67.12795887299539\n",
      "athles tins wiod richmans.\n",
      "\n",
      "BIs tr GII:\n",
      "Kin thou shwart un my leagnTmink hin con tomy dobtht thy hile\n",
      "LOSS = 125.56991730564908\n",
      "an toughs wret docnise kave sin hm zays my lesle and and anvery fave\n",
      "Wast eavesuls!\n",
      "\n",
      "DUCNLN Y II:\n",
      "Why\n",
      "LOSS = 90.14588228776195\n",
      "ad that sevich me blorde.\n",
      "\n",
      "KING OICHARD II:\n",
      "Fow GontXunt of hach on!\n",
      "As sceps forot that calcef inkla\n",
      "LOSS = 90.27072313065729\n",
      "and, wis wame rroudss,\n",
      "As hin a memfees den he rened\n",
      "looun dears ofow rie ssalver one buthy slaudne,\n",
      "\n",
      "LOSS = 95.03019053815036\n",
      "at Oy parteder doucht lord Hacke durgent'd goregs\n",
      "jndse inont apd hirX netd and lutKese of Llongs\n",
      "Anc\n",
      "LOSS = 111.86496291937753\n",
      "are to soll nagth to shake his glondow wath us, fate ties,\n",
      "Se forothinate fard:\n",
      "Gouy, woad How, and k\n",
      "LOSS = 98.77732545542273\n",
      "a-''ghing;\n",
      "BuRkeror tith enccolter\n",
      "Thinc, the wall.\n",
      "\n",
      "NORKY YOLDUNY BIHERLDuRD:\n",
      "Boughing to nocbus? al\n",
      "LOSS = 114.38845149357148\n",
      "acut, the vencle-d he daytory,\n",
      "Ttre with merthicus?\n",
      "\n",
      "HOND BOKLYA:DFOB:\n",
      "Me frostinot ale o youre, come\n",
      "LOSS = 101.24743344476293\n",
      "awers, and sindre of to arand ate ase entons offreast's farsh nokr gruches:\n",
      "NorZhird\n",
      "\n",
      "KING RLINBAE:\n",
      "L\n",
      "LOSS = 90.84875302397491\n",
      "auth; the hev reswelly\n",
      "Wall dow whol ary roghbef: thee qoow, in cuntothtsue.\n",
      "\n",
      "Vred frous ake codilan:\n",
      "LOSS = 92.70509654217419\n",
      "ad talliding and lok.\n",
      "\n",
      "FONT ROKHARLE:\n",
      "Gow: I mall fouren sndames\n",
      "Fnis tele sroy hasterngaPtire,\n",
      "And s\n",
      "LOSS = 89.42408368509436\n",
      "and thet yeveamare, the raath gongool so whate-s fow,\n",
      "Heal sha! saw, sple this, of walven reapce thou\n",
      "LOSS = 103.1195257731302\n",
      "atred, theoph;\n",
      "The gucost or this spriming eiwer proon tily.\n",
      "\n",
      "Ild my that the ERrand of prody.\n",
      "\n",
      "FOrd,\n",
      "LOSS = 100.83669068301784\n",
      "at be cullmom heejest anco thin to me dains, whould kirts stey.\n",
      "Thou arin thou in tere;\n",
      "In hands lard\n",
      "LOSS = 108.35478293084961\n",
      "and herys paree, strouch; coul this youl sene\n",
      "Tpren wemer destiman, moly,\n",
      "Noush be ringed, in the for\n",
      "LOSS = 83.22217842438833\n",
      "anthentwoull; I all vencemfirk the owa my prest Bath me.\n",
      "This folk Bonglagung cowll have\n",
      "de gaves me \n",
      "LOSS = 90.35828816305131\n",
      "arZ\n",
      "Qnginst,\n",
      "Nord mechild with my bay coprowing SARkHAY:\n",
      "Eg,\n",
      "The we aering ous?\n",
      "\n",
      "LOUEH OUUO\n",
      "Aw RyRKAN\n",
      "LOSS = 74.15286362414888\n",
      "ajce pole graile,\n",
      "That scolst,\n",
      "To than comrend-, porther noth:\n",
      "Whis berome hill vottoe not to leadon-\n",
      "LOSS = 90.95695173509587\n",
      "at me the mate be mewer sool.\n",
      "\n",
      "DUKK OF YORK:\n",
      "\n",
      "KIHG RICHERD II:\n",
      "\n",
      "DUKE OF YORK:\n",
      "Mawe of mango, hall tho\n",
      "LOSS = 102.49891793644629\n",
      "and year Rreward icle aid arpard dreanh;\n",
      "Sormang, be of his tousev-ond sorpastaid I to he dory; wech \n",
      "LOSS = 95.59801350969008\n",
      "ard,\n",
      "Mlenet me conwtinitiand, my tins a plake, the aleard\n",
      "And freatent's dath,\n",
      "That holl ofure astast\n",
      "LOSS = 98.60528972253998\n",
      "at what shald's at of the my's nivees a tert-.\n",
      "Say a prewce sbabr where petier you hat ligug ar;\n",
      "Shav\n",
      "LOSS = 94.81669408288448\n",
      "as ond cont by byansX's hemeang.\n",
      "\n",
      "GREEOF EOL:\n",
      "It dom, speversorod.\n",
      "\n",
      "AEMEOR:\n",
      "Apaod; you my her'sil qou\n",
      "LOSS = 107.5510794273737\n",
      "at;\n",
      "Buck mid beniguchert-$y the gening whot mive I nobrept;\n",
      "Gust wore ten this pealereuince\n",
      "Mow!\n",
      "Then\n",
      "LOSS = 107.26840814567583\n",
      "atted\n",
      "Preak harg mespo thou to ile, gay with nest agetino addent at,\n",
      "GoRe hats lovelG, you crouting g\n",
      "LOSS = 72.16592820766077\n",
      "a.\n",
      "\n",
      "PERPMI::\n",
      "Abring seve er fatereshe. eresteess muad?\n",
      "Ar I no go dowe.\n",
      "Pour vig? yousir winsineds?\n",
      "\n",
      "\n",
      "LOSS = 106.97641730829287\n",
      "ant thast the woote, yo bescole, dungis us the heart:\n",
      "The tonow Is bool woogte for: un this lceily we\n",
      "LOSS = 92.15179472470878\n",
      "amone and bemat a noveed a will gnlimes,\n",
      "To feman lemy hoth.\n",
      "\n",
      "Buthe:\n",
      "What ondhees, loveuve hil I bess\n",
      "LOSS = 72.98907490681856\n",
      "arll,\n",
      "I wnot, the laptios. by ere I to nit hive there hy, they far is ath,\n",
      "Mearhsice thee dreews beth\n",
      "LOSS = 93.94580603557779\n",
      "at you layst! anm Sime wirtfiest a floull pwoos.\n",
      "And if to ming,\n",
      "It in my brom kight wors.\n",
      "\n",
      "RUMEO:\n",
      "Mo\n",
      "LOSS = 98.10753933074444\n",
      "artesen is a then throth chan may deat as my ofpil ts Wape, to kinuegt.\n",
      "\n",
      "ROMOO\n",
      "Thy, should love motti\n",
      "LOSS = 77.24704657920134\n",
      "ar?\n",
      "MOr I strove puite:\n",
      "I were that nies of a bray.\n",
      "\n",
      "RORCO:\n",
      "I be: the thene of isty darse.\n",
      "\n",
      "RiMEO:\n",
      "Co\n",
      "LOSS = 87.69197783971505\n",
      "aze, It in that;'?\n",
      "I the dewturs mare would and, is the keparen'd, deveraigh,\n",
      "Bet's but qow in?\n",
      "Hewai\n",
      "LOSS = 107.03829737583699\n",
      "any\n",
      "Whouborter thy mesinnod: ning, the shils of she pancel;\n",
      "I hild rae hat the prom, thee shilk quan-\n",
      "LOSS = 85.58829692661355\n",
      "ast utpue, Gothad, goth not or love nith.\n",
      "Fo thy mans, cacmor.\n",
      "MERCCALOO:\n",
      "Ton sumee hour me.\n",
      "\n",
      "BONCOLI\n",
      "LOSS = 109.5065507881275\n",
      "ad on Momereris?\n",
      "Ast love, whe doaght in steads?\n",
      "Hhourwhen Fordhen and hango-\n",
      "Men readse.\n",
      "I, thot tom\n",
      "LOSS = 112.9539201796561\n",
      "atespn dorced, plast doup no feast ele.\n",
      "\n",
      "RAMEO AO\n",
      "WhERCCATT:\n",
      "P, dy alt your tINaid promn thou ir is n\n",
      "LOSS = 93.16399545104377\n",
      "at, but thou wich and,\n",
      "And ceoyh twean, head!,\n",
      "Cise by sue, thue miet,\n",
      "Cirty, gust in thinge, thy fai\n",
      "LOSS = 89.74219532235004\n",
      "are toure.\n",
      "It bucj in the will len to tting are a noght.\n",
      "\n",
      "Nurse:\n",
      "Uhare thy anfer thbe awisge ustleses\n",
      "LOSS = 89.4940362652967\n",
      "at thou vertllmpet a bin;\n",
      "I wilt, the womery for wonvurt'ack;\n",
      "Qnave, cor, sle, me that noth ta emsh, \n",
      "LOSS = 83.35935072146721\n",
      "atallled to my proisern?\n",
      "Culse, for kingedhefs; fart now wase py lides, I'll eyver beserele lovesgris\n",
      "LOSS = 119.4194600232605\n",
      "armead stray lemele apreckmor'-.\n",
      "\n",
      "JURIEN:\n",
      "Spefcest,\n",
      "Mast thy here that your manks therebagt sowlee.\n",
      "S\n",
      "LOSS = 73.22066376796323\n",
      "ato!q-RID:\n",
      "Nour, thee shall fare;'-f Duave,\n",
      "Thy savet thAm inTeef-ihy an: cod at love.\n",
      "Amay no; shay \n",
      "LOSS = 91.5704029542581\n",
      "abrea; nos my say! dea's hiden lack and yur is?.\n",
      "AK is to I lave has po bove soalowte, and and Vouth,\n",
      "LOSS = 97.26153217093618\n",
      "ang we theble a diesy Mind it weul mistein. Whysere night of hountt,\n",
      "For this, noth, I for strie neve\n",
      "LOSS = 116.88934551930166\n",
      "ay:\n",
      "And shwelt is bagigreTat:\n",
      "What is chouhe mou, say liget thom notent I sumfile, I'll thou prion th\n",
      "LOSS = 109.599790740301\n",
      "ad:\n",
      "Her cull death?\n",
      "Ho hows: try blay;\n",
      "\n",
      "Nech QAURENTE:\n",
      "Dome; Jilling knood.\n",
      "For himar?\n",
      "Ewil there wil\n",
      "LOSS = 80.53056735312293\n",
      "ate-tencher dewbed.\n",
      "\n",
      "RRDET:\n",
      "The grinedst and it me on to the licounds\n",
      "Cith blied.\n",
      "\n",
      "GRICCE:\n",
      "Of so mish\n",
      "LOSS = 108.68823831918\n",
      "atn\n",
      "He suref O sorsess, -lothing still gozes-.\n",
      "\n",
      "JQLIORN\n",
      "And an now the ackand\n",
      "Where proosuine bondy e\n",
      "LOSS = 104.22004066539505\n",
      "an thou nouse Fird and Anbaght,\n",
      "Whan wo the myn conoold to jon;\n",
      "When Cling come us heren and Painith\n",
      "\n",
      "LOSS = 78.0179562886221\n",
      "a not the. I hap lisgnited firfe reef and.\n",
      "\n",
      "ETIC:\n",
      "And, sont! wangcaince:\n",
      "To deay I ,lawous dide thy s\n",
      "LOSS = 106.94143007733194\n",
      "arised reses, dy fother we fitht, shivy bly!\n",
      "\n",
      "ELFTCY I vis a I an rese, O d, greath seal ki,\n",
      "As a gif\n",
      "LOSS = 94.53907501408679\n",
      "alth.\n",
      "Wilikers's holdicks thing orfulds; rese wire thee so mair willd\n",
      "With he comain thy this. of win\n",
      "LOSS = 86.91035894239887\n",
      "ansay in out the Gon?:\n",
      "Thy mes, my, platke you lown tatllarsh? slead hay so tearer-m my owferent Nord\n",
      "LOSS = 97.22146120076526\n",
      "and,\n",
      "Bysee cengnersomonay?\n",
      "\n",
      "MFCAURED;\n",
      "Whee brear of Nount would geavellay I see Curin.\n",
      "Seaven on thee\n",
      "LOSS = 78.1684545251267\n",
      "arkugick Hord's death allsgh,\n",
      "Son there these!&A, he which apay frogger, maline reat,\n",
      "Uncell hay Mult\n",
      "LOSS = 87.44733738602075\n",
      "ank of blamens ton.\n",
      "\n",
      "RDWARDE:\n",
      "Sheareer thar seen thy be thy ghath henger.\n",
      "\n",
      "UUREW:\n",
      "Why heake, may that\n",
      "LOSS = 113.61843051709967\n",
      "auth these sonton.\n",
      "\n",
      "KING HENRO vIN-NIND:\n",
      "Wizen thee to my be the to con, rerland,\n",
      "Away cieith the hea\n",
      "LOSS = 95.41073833618287\n",
      "ars men if broon,\n",
      "She thy for my life\n",
      "But mine make hay nour gidn biged his gower:\n",
      "Kose soun father h\n",
      "LOSS = 88.93997835151589\n",
      "ar suepart.\n",
      "Vhand usher, theniYent the sroza;\n",
      "Ming hend it the fraisp I rrofe and nee,\n",
      "Mord lest my t\n",
      "LOSS = 103.3839693191919\n",
      "ad, it gome, kence:\n",
      "Why slay, he o man eetten.\n",
      "\n",
      "Hestear:\n",
      "Here lith anow es thinks tall flom of all\n",
      "As\n",
      "LOSS = 63.30684492641548\n",
      "and, that, if I deather.\n",
      "\n",
      "MDDY GREWS:\n",
      "Vo shiely god, Norishit boom; kith thison, ar not, I have broon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS = 91.33872975959227\n",
      "as thing;\n",
      "In to the chanipes be angaut, and, a'le one, is that grain'! both bece lemperenok\n",
      "And misth\n",
      "LOSS = 70.91920291339164\n",
      "adin of stit are not susfue prouken hen countel\n",
      "\n",
      "EDCARD\n",
      "AKy linfulous, O,\n",
      "IUGBECESW:\n",
      "Muce that foimis\n",
      "LOSS = 100.22355616976814\n",
      "a thy betf mes,\n",
      "Envleds. Wadwet qualling for pleaFe,\n",
      "I wiloog Esward canricr gowe.\n",
      "\n",
      "LARTIN EARY VETIS\n",
      "LOSS = 75.3252482830311\n",
      "ard we geman!\n",
      "\n",
      "KING LDWARDM:\n",
      "Theing with the love yout dailorle westite,\n",
      "Bot des, batht theel awe: an\n",
      "LOSS = 85.65568189112648\n",
      "and you will pakst the lord,\n",
      "But he heir's sotherd's for where our forged holf\n",
      "Eethy frience, wildeve\n",
      "LOSS = 110.8644832308796\n",
      "ands to man lord?\n",
      "\n",
      "KING EDTARD Evear then what wasted Fear;\n",
      "You houshve me tower party $irtires,\n",
      "\n",
      "GDL\n",
      "LOSS = 101.2849402811327\n",
      "arty,\n",
      "Words, blt youm, for there's my hosh poffore,\n",
      "And conyht frile me at plan mere the caknd.\n",
      "I hat\n",
      "LOSS = 84.0417450377906\n",
      "arts;\n",
      "Far you:\n",
      "Till be ent tink him the fateateves betis down which: hour fold.\n",
      "O play awfill, conlot\n",
      "LOSS = 78.51915439964743\n",
      "att ruth ut honf have?\n",
      "\n",
      "GOENCED:\n",
      "Atwick the wear hond you kondenoule, of in a dospo, and praile.\n",
      "\n",
      "GOE\n",
      "LOSS = 103.4960261749228\n",
      "ars if ace;\n",
      "That owered that larsay caurths.\n",
      "\n",
      "KING EDWARD IV:\n",
      "We farkn frod well are bead and wart Bo\n",
      "LOSS = 87.8165345335708\n",
      "air:\n",
      "There's thenk, tace soungre trenty'd;; yeinse seel the fear heng.\n",
      "\n",
      "QUMENSEORBEN:\n",
      "Whe groning wit\n",
      "LOSS = 103.69265117907824\n",
      "als horse my glood'ss sis ewry reep.\n",
      "\n",
      "EDwaRD alaince's weem.\n",
      "\n",
      "GLOREECEAR:\n",
      "What, weRh the ipersped and\n",
      "LOSS = 94.96254139839786\n",
      "an; ow spread,\n",
      "Sit sing that bores nop the elP?\n",
      "\n",
      "SLOUCESTER:\n",
      "Whow's love to the coulverestill wespeat\n",
      "LOSS = 95.37982505233568\n",
      "an, IUlese, bet;\n",
      "Oh, fear to lifder', in of the mad in you not\n",
      "For ell with up his.\n",
      "\n",
      "GORCEENE:\n",
      "Yere: \n",
      "LOSS = 107.55992779883674\n",
      "ars, ig Kinger, I strilemield am, doon my my crbeweltio,?\n",
      "\n",
      "LOLITESSHU\n",
      "Showe me, for promenty.\n",
      "Gospant\n",
      "LOSS = 82.56514599570457\n",
      "at it triindonesis an.\n",
      "Fat thou you the slyut bess all bovives Shall? a tott, thou samer's bur bewire\n",
      "LOSS = 93.2829627784458\n",
      "ay?\n",
      "\n",
      "LOOLIESE:\n",
      "O agauthoud fooh itlifet:\n",
      "But ladsus!\n",
      "'thighingd ame the, you toum.\n",
      "\n",
      "LAOVII:\n",
      "Shall be \n",
      "LOSS = 77.62748515214548\n",
      "an you, your e's but fordo.\n",
      "\n",
      "MOCCALINRS:\n",
      "Let nich's are\n",
      "The smack fatless with ot\n",
      "And a betcaghtos\n",
      "Ho\n",
      "LOSS = 86.76224770069882\n",
      "as the, and your will his speoct, I cam?\n",
      "\n",
      "BENMCOONLOR: The onf with her dray\n",
      "As is duringd.\n",
      "\n",
      "HERNYONE\n",
      "LOSS = 86.53149059993896\n",
      "ard your ackbing,\n",
      "In vastink as in ormy innorignst!\n",
      "\n",
      "PLIIIA:\n",
      "I do this stillfok her be monty\n",
      "Nour ben\n",
      "LOSS = 102.92931400586572\n",
      "andost I gandy\n",
      "He saigious, he if shour stear\n",
      "Bur of strence wafpenired erih our all ando-\n",
      "This groug\n",
      "LOSS = 89.5210543110834\n",
      "agh and had:\n",
      "Me, dray not sain agraling butt-'dse insome and;\n",
      "Lost!\n",
      "Some and ender will:\n",
      "Cbeel, saive\n",
      "LOSS = 93.28389889579189\n",
      "at no have me: my lord.\n",
      "O such To a soyed;\n",
      "Him, my levine\n",
      "I prien your pomady oor, your, be ancant'd,\n",
      "LOSS = 98.65683759350712\n",
      "ak with broum, and then the with apy\n",
      "For lo dork so pity me.\n",
      "\n",
      "LEONTES:\n",
      "Thy greacine seeg mets at to b\n",
      "LOSS = 140.33598763655084\n",
      "al of threcoud; as year's whis braini,\n",
      "This gooly abe!\n",
      "\n",
      "HERIMEN::\n",
      "I dois Here ruestin it, as vife to \n",
      "LOSS = 101.15411942907946\n",
      "arume a tout\n",
      "Riny, call fost nothems. wemmingst by ye' awine ading\n",
      "thy sint string'd mentpereanit's\n",
      "-\n",
      "LOSS = 99.00500819753742\n",
      "all Poighers my dast and everf it halaid wouth lide! not that\n",
      "I lave feay, my sepen man so a vord.\n",
      "\n",
      "C\n",
      "LOSS = 107.83773719877816\n",
      "ace\n",
      "O't now shay in that the tay lifed\n",
      "Whone, with me deast? 'tuse Nock whle uphise:\n",
      "to me\n",
      "For hall o\n",
      "LOSS = 75.9300422057356\n",
      "an! I no nemes shilt of shefe wo,\n",
      "Ththe caloing live fraodel, to thing our grecient.\n",
      "He send no them \n",
      "LOSS = 99.75350773411051\n",
      "at, ond mine, your gon:\n",
      "Swe is not samby, but secl one brosm pre sing\n",
      "wis the thought untlesst abe.\n",
      "\n",
      "\n",
      "LOSS = 90.81005990573172\n",
      "ay darce look:\n",
      "Espeast with sweet or the ham and such.\n",
      "\n",
      "PRLIXES:\n",
      "Yo, was speast you, him latelare-st.\n",
      "LOSS = 92.6370609337639\n",
      "ad prave engerepfullunt, hor to not I care\n",
      "Sare and man us this far thy neclose a hingafra must saity\n",
      "LOSS = 95.69077951213507\n",
      "atheit, word and, you shave now have benot of or madluedst;\n",
      "we cautticco ip amdan a fring in hord tha\n",
      "LOSS = 101.74613792413805\n",
      "a misherite treen\n",
      "All wushowner haw we beyout my love. fatire you; lidessess; your dogare more, me co\n",
      "LOSS = 101.55215740592897\n",
      "ading must home thengeran pricks'd ap't spording.\n",
      "\n",
      "LLORCES: que? he wnow'd endand that and pam\n",
      "And, t\n",
      "LOSS = 82.08015720639311\n",
      "aves to more himsion,\n",
      "the eyegee Ash tly is eyenf--ore mefrowan?\n",
      "Purso in ereer this I am pless for'd\n",
      "LOSS = 102.2383332015895\n",
      "ax,\n",
      "My rashach you so mice to wice\n",
      "For ald him: do nesk\n",
      "I some and fase stals-s, the wnigg, thein and\n",
      "LOSS = 106.16014364911494\n",
      "ach him; the lestt all is for thalt couce\n",
      "and whop. I noul her: was pose and such cannes, he wears wo\n",
      "LOSS = 96.31742663207433\n",
      "ane\n",
      "So anleess soy, mut sour deit of gonger: storghed\n",
      "And the moness, apast pay the lither asserwer\n",
      "A\n",
      "LOSS = 90.92874504214119\n",
      "auld, Iw, as Clome. ALiys uf on kneddess perifain:!\n",
      "She haw was I have steal toun\n",
      "\n",
      "AULONAAE:\n",
      "Enily: a\n",
      "LOSS = 102.55753785141562\n",
      "ake you wewpues corthen, vere\n",
      "I would yourwill.\n",
      "\n",
      "AUTOLN:\n",
      "You, not me kith you day! heard'd\n",
      "Bit, with \n",
      "LOSS = 97.87917511187962\n",
      "ad.\n",
      "\n",
      "SIMAO:\n",
      "Goshe, more heard,\n",
      "Or dobledgataint you strount of nouster, of will:\n",
      "Werew with the vary.\n",
      "LOSS = 87.73678959923603\n",
      "and man withs your hathads I have to with their\n",
      "nige, and would there glincoie.\n",
      "\n",
      "CLARDIO:\n",
      "I than moye\n",
      "LOSS = 86.18941506769356\n",
      "ayst spefach and a traregn\n",
      "Aupo so.-\n",
      "A plost blow, in o? steen for stress owhees; had; and the qoyeat\n",
      "LOSS = 64.24292971590899\n",
      "asted, that, dive are oofor.\n",
      "\n",
      "CUMIL:\n",
      "Her; the eyhinkuce, you-hry meding hing as ste is we eeffery.\n",
      "\n",
      "W\n",
      "LOSS = 52.90526331236651\n",
      "asparnted demps fiablost sim\n",
      "Which you willfull; an.\n",
      "\n",
      "POMPEY:\n",
      "Indeice thin?, werd's veil,\n",
      "Antery kild\n",
      "LOSS = 73.6420891369391\n",
      "at us iswa.\n",
      "\n",
      "ANGILLO:\n",
      "As take that it not de; offt:\n",
      "To I bontery on more Bumon\n",
      "this to be thou as may\n",
      "LOSS = 89.91388326517894\n",
      "a!:\n",
      "Are more say tingess, mown mier.\n",
      "ULond I an once, you bryor, thou dish in busting to wither parma\n",
      "LOSS = 124.73594940407781\n",
      "annen serlond.\n",
      "\n",
      "ISABELLA:\n",
      "Ed I, by to mother, and shall when bece all umait is somp'd then\n",
      "Is an hear\n",
      "LOSS = 114.15279802181625\n",
      "ar hooour and who.\n",
      "On you lother: fathe, my all a blisecy come,\n",
      "And losh you'ly torn avingw't.\n",
      "\n",
      "ANGEL\n",
      "LOSS = 98.17146355609306\n",
      "af!\n",
      "Tell of I ladely diemon me, weran long''s tParcent.\n",
      "\n",
      "CKAUDIS:\n",
      "The and plawe, the dost is,\n",
      "-rew by\n",
      "LOSS = 105.8816617924476\n",
      "ard forth, shiflet to spoul.\n",
      "Se; led her his live I'\n",
      "TI hear world mishe, blaw he lister to thy panno\n",
      "LOSS = 100.55556476193885\n",
      "anses, brothink, will speet, awent the evvice, see\n",
      "poft thiy bepist, it say.\n",
      "\n",
      "CLIUD:\n",
      "Gon's, he pay. U\n",
      "LOSS = 101.3587119631819\n",
      "ad thy buss the geist your the besteld.\n",
      "\n",
      "DUCIO:\n",
      "I man tom, gnormpracian! bety anguiro a indowo; calli\n",
      "LOSS = 91.43345131511526\n",
      "akm the Tust naje\n",
      "traiser.\n",
      "I wasene for my both than muse the\n",
      "grair, anj zowy, ork lick ofay with dea\n",
      "LOSS = 100.56947671203216\n",
      "ans, fould, you, cit;\n",
      "Ofr, all'dftornestoffandss, to bucale age of friencticest of I\n",
      "me! the pityes a\n",
      "LOSS = 84.33358308516179\n",
      "ald he well witherselis yit of the accured preart.\n",
      "My the be to life seget to all to connought lord t\n",
      "LOSS = 90.93932084832899\n",
      "as am sterverade-\n",
      "Ford his awhare. Cat hes; him nop:\n",
      "If will I sirity? the pramed his metturm\n",
      "thom hi\n",
      "LOSS = 91.83240603089479\n",
      "and the swenter can the werd\n",
      "In his night spath eved itsulchely tomes nou; his aukef, nath we hast wi\n",
      "LOSS = 100.2440862737651\n",
      "alvangs,\n",
      "Awseaveds, bave on shalmenfs\n",
      "That woold mainty, 'to he wake but and of the farse, prochire m\n",
      "LOSS = 92.49466777022697\n",
      "ay to shall\n",
      "Xongease terde, you agily. You crain.\n",
      "\n",
      "HANTIT:\n",
      "I seemftime; I vartite yet with sever him \n",
      "LOSS = 95.24719604485917\n",
      "ands of cusion Perepted.\n",
      "\n",
      "HARIANA:\n",
      "I lest this her! thou his promaceys; it infercas! what you\n",
      "how let\n",
      "LOSS = 76.11164342694678\n",
      "ard,\n",
      "I did come your fayertignes,\n",
      "'t shond man my must this insed, him's the respemantit now she pild\n",
      "LOSS = 87.74109100652107\n",
      "at ice be man stits him ir bed abeared:\n",
      "No he's hibet attitty,\n",
      "Lord art vartiar to stake and come can\n",
      "LOSS = 92.81442657126226\n",
      "anted.\n",
      "\n",
      "Lord:\n",
      "You plosveroundeld.\n",
      "Yauk unco demy may haw for's,\n",
      "S3orr, you know'd?\n",
      "\n",
      "LUCIO:\n",
      "Soor noser\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS = 99.23606590461878\n",
      "actest your knace anband\n",
      "And his somh that the rays than abright,\n",
      "Go vauring hiv you, sarry to come,\n",
      "\n",
      "LOSS = 96.04412104598529\n",
      "ant, you with ableith,\n",
      "For think shill a to this to som\n",
      "\n",
      "ALUO:\n",
      "Andil the you have sceecio snive! Fill\n",
      "LOSS = 84.70988132260881\n",
      "ack, in thusse3, hadded known\n",
      "I though, Tave in Cugte, and obe my gnang grave.\n",
      "\n",
      "PABERL:\n",
      "I thenk I hav\n",
      "LOSS = 79.20476656319217\n",
      "ave for man it a mains of hath', in cannolr am;\n",
      "Sine, I such qlanftyst carnys you thou Viss!\n",
      "Spore my\n",
      "LOSS = 98.2394395504418\n",
      "ash shill Loke,\n",
      "Lea to tilce.\n",
      "UURTONTOO:\n",
      "Sair I have lot? fille anse man I surks?\n",
      "Sis, with they and \n",
      "LOSS = 81.86519219748068\n",
      "ant, the word,\n",
      "Bepound, I for behto lovedencian!,\n",
      "Well, you it not wire! For hissels of sir.\n",
      "\n",
      "TRAMIO:\n",
      "LOSS = 78.3501227719975\n",
      "auket! the not, I give with udpercoue:\n",
      "Pan the juty,\n",
      "I baor and sirstiino, maid sabe that her all?\n",
      "\n",
      "M\n",
      "LOSS = 120.86425718900092\n",
      "aly\n",
      "think I know leas trunction or steouder wooNnt:\n",
      "Wey in very and blop tize our evelver caustrunt m\n",
      "LOSS = 91.25513720912541\n",
      "ant, wall in, and know shall well ane love flome.\n",
      "\n",
      "YORKERT:\n",
      "Aspearn! there't swants.\n",
      "\n",
      "KATHARINA:\n",
      "Whis\n",
      "LOSS = 95.73639597881845\n",
      "all:\n",
      "Then is stenn she are have you will confuchol, Lay, than I mate deach would hit yet yeascibe inw\n",
      "LOSS = 86.89397925126576\n",
      "aid cayoun's this honest\n",
      "So be, fat he and call my cilv of hand:\n",
      "This it do o-fer tul\n",
      "Sis 'teng that \n",
      "LOSS = 107.33602077675974\n",
      "as, take this smal' it tell he\n",
      "Morst: I in it:\n",
      "But, your there well Romb-brange world gid:\n",
      "Both you b\n",
      "LOSS = 84.53802101146385\n",
      "ath,\n",
      "here, he have nover hanged foushrest a prawens should her:\n",
      "Threporten untost o'd for,had liefe; \n",
      "LOSS = 92.49345042733685\n",
      "ay rister vire I\n",
      "Anbyellow talving, theech thou to\n",
      "Caliol ture\n",
      "hir, a falmed,\n",
      "Now liese o'll have she\n",
      "LOSS = 77.96291972934249\n",
      "and wive? As me sur, shall unservy.\n",
      "\n",
      "PETRUCHIO:\n",
      "Wheis?\n",
      "'etull thou that imen that? Shere comur you me\n",
      "LOSS = 93.76326041703523\n",
      "arce the? and not.\n",
      "Cometa uncelienio; be\n",
      "Anawly you fare my ghothers Wather.\n",
      "\n",
      "PAPTINCA:\n",
      "Cheps be!\n",
      "I'l\n",
      "LOSS = 99.82063977381603\n",
      "and one to mis that ave a mustal'd\n",
      "Pain, of muchh!\n",
      "Have se naailock of spepan thee mined withte mysel\n",
      "LOSS = 73.48293376973322\n",
      "and so lud!\n",
      "\n",
      "BiPcEETIO:\n",
      "I'll and shall you that in mad have pare and'd here my love:\n",
      "Here of wnore I \n",
      "LOSS = 87.21643028318059\n",
      "adimy brisper mistren, be aid; tees.\n",
      "\n",
      "BETMENHA:\n",
      "Had piar all sir.\n",
      "\n",
      "GRUCHIO:\n",
      "Her, sare's perlio, here \n",
      "LOSS = 68.48516363628245\n",
      "an me were sior;.\n",
      "\n",
      "CTRTIS:\n",
      "Prusumadell you, know a lain to housine is it.\n",
      "\n",
      "GRESIO:\n",
      "Whon, a bath not\n",
      "T\n",
      "LOSS = 95.45218507317766\n",
      "ant,\n",
      "Well ap besseis therefareiscow for like.\n",
      "\n",
      "BAPTISTA:\n",
      "Comind thee now in oftiss in sentio;\n",
      "Leder i\n",
      "LOSS = 86.75566940334289\n",
      "ar; I that ane husbak as staw?\n",
      "\n",
      "RICHRCHIO:\n",
      "Pay, 'tile at sheachs for beson.\n",
      "\n",
      "KATHARINA:\n",
      "She namior of\n",
      "LOSS = 89.43500308270069\n",
      "aw till my twerret you will with,\n",
      "It me ont your wists chought!, thou goed clees to-se:\n",
      "Pir, sirverst\n",
      "LOSS = 83.77583285068252\n",
      "aling oet unfoling Cuxpalure the grodned\n",
      "Lodst me.\n",
      "\n",
      "PORIANATU\n",
      "Hore you neverted and sontorned, am edr\n",
      "LOSS = 84.82983195192242\n",
      "ady\n",
      "Of thm great have I'll and fearth,\n",
      "Which him yourseet'd dive come liesh.\n",
      "\n",
      "ROMIO:\n",
      "A than, be reatu\n",
      "LOSS = 94.67417514022722\n",
      "azent for exAr!\n",
      "Bast firs filts everman! ned I cament not deat saiked:\n",
      "Do the dight\n",
      "On ours and thy b\n",
      "LOSS = 80.10378243265781\n",
      "and what the dest me?\n",
      "\n",
      "MISANDA:\n",
      "It fare:\n",
      "I maromrerniess, sir,\n",
      "My dassin\n",
      "Aur: O take as for more. And\n",
      "LOSS = 75.35087310455793\n",
      "are that mapbied gallip:\n",
      "I come? gelathres-winge of him?\n",
      "\n",
      "GRONAO:\n",
      "As, in reed,\n",
      "All misle virte!,\n",
      "Disp\n",
      "LOSS = 65.13507172024157\n",
      "att,\n",
      "An julkeedinc and firl cliams on thou wendstercuso\n",
      "thou penchy of that jovio-s is.\n",
      "\n",
      "HERTANIO:\n",
      "Wi\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Units (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
